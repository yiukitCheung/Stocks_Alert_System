{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer, Consumer\n",
    "import confluent_kafka.admin\n",
    "import yfinance as yf\n",
    "import json, schedule, time\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient, DESCENDING\n",
    "import traceback, logging\n",
    "import sys, os, yaml\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Construct the path to the directory containing the module\n",
    "module_dir = os.path.abspath(os.path.join(current_dir, \"../config\"))\n",
    "\n",
    "# Add the directory to sys.path\n",
    "sys.path.append(module_dir)\n",
    "\n",
    "from data_pipeline_config import load_pipeline_config\n",
    "from mongdb_config import load_mongo_config\n",
    "from kafka_config import load_kafka_config\n",
    "\n",
    "from confluent_kafka import admin\n",
    "import confluent_kafka\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level to INFO\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  # Set the logging format\n",
    "    handlers=[logging.StreamHandler()]  # Add a stream handler to print to console\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Data Extraction and Streaming with Kafka and MongoDB\n",
    "\n",
    "The `StockDataExtractor` class is responsible for extracting stock data, processing it, and streaming it to Kafka for further use in downstream systems. The class supports both real-time streaming and batch processing by interacting with Yahoo Finance, MongoDB, and Kafka. The key functionalities include:\n",
    "\n",
    "- **MongoDB Integration**:\n",
    "   - The class initializes MongoDB connections and manages both streaming and warehouse collections for storing historical stock data and real-time data streams.\n",
    "   - Time-series collections are created to store stock data efficiently with options for expiration to control data retention.\n",
    "\n",
    "- **Kafka Streaming**:\n",
    "   - The class uses Kafka as the streaming layer. Topics are created dynamically for each stock interval, and stock data is sent to Kafka for real-time consumption.\n",
    "   - The class supports deleting and recreating Kafka topics if needed (e.g., during a reset).\n",
    "\n",
    "- **Data Fetching**:\n",
    "   - Stock data is fetched from Yahoo Finance for both real-time streaming (using small intervals) and batch data storage (using higher intervals like daily or weekly).\n",
    "   - For streaming, the class fetches the most recent data based on the configured time window or the last fetch record.\n",
    "   - For batch processing, the class ensures all stock data up to the current date is captured and stored in the MongoDB warehouse collections.\n",
    "\n",
    "- **Data Processing Pipeline**:\n",
    "   - Data for each stock symbol is processed by fetching historical data from Yahoo Finance, preparing it, and then streaming the processed records to Kafka for real-time applications.\n",
    "   - After processing, stock data is stored back into MongoDB for historical record keeping, enabling efficient querying and data retention.\n",
    "\n",
    "- **Scheduled Data Fetching**:\n",
    "   - The class uses `schedule` to run periodic data fetching tasks during trading hours, ensuring continuous streaming of stock data in small time intervals (e.g., every 5 minutes).\n",
    "\n",
    "- **Configuration Management**:\n",
    "   - Configuration files (YAML-based) are used to manage the system settings for data pipelines, such as intervals for streaming and batch processing, Kafka configurations, and MongoDB settings. The system also tracks the last fetch time for each stock symbol, enabling efficient data fetching without redundancy.\n",
    "\n",
    "This class efficiently handles the extraction, transformation, and loading (ETL) of stock data for both real-time streaming and batch processing, providing a solid foundation for a data-driven trading system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataExtractor:\n",
    "    \n",
    "    def __init__(self,symbols,mongo_url,db_name,streaming_interval,warehouse_interval,kafka_config,data_pipeline_config):\n",
    "\n",
    "        # Set the reset and catch up flags\n",
    "        self.reset = data_pipeline_config['data_extract']['reset']\n",
    "        self.catch_up = data_pipeline_config['data_extract']['catch_up']\n",
    "        \n",
    "        # Set the class variables\n",
    "        self.symbols = symbols\n",
    "        self.current_date = pd.to_datetime(\"today\")\n",
    "        self.last_fetch = {symbol : {interval: None for interval in streaming_interval} for symbol in symbols} if self.reset else data_pipeline_config['data_extract']['last_fetch_records']\n",
    "        self.streaming_interval = streaming_interval\n",
    "        self.window_size = data_pipeline_config['data_extract']['window_size']\n",
    "        self.expireAfterSeconds = data_pipeline_config['data_extract']['expireAfterSeconds']\n",
    "        \n",
    "        # Initialize the MongoDB client\n",
    "        self.client = MongoClient(mongo_url)\n",
    "        self.db = self.client[db_name]\n",
    "\n",
    "        self.warehouse_collection_name = self.warehouse_topics = [f\"{interval}_data\" for interval in warehouse_interval]\n",
    "        self.streaming_collection_name = self.streaming_topics = [f\"{interval}_stock_datastream\" for interval in streaming_interval]\n",
    "        \n",
    "        # Initialize the Kafka producer\n",
    "        self.producer = Producer(kafka_config)\n",
    "        \n",
    "        # Create Time Series Collection if it does not exist\n",
    "        self.createTimeSeriesCollection(self.streaming_collection_name, expireAfterSeconds=self.expireAfterSeconds)\n",
    "        self.createTimeSeriesCollection(self.warehouse_collection_name)\n",
    "        \n",
    "        # Delete kafka topics\n",
    "        self.delete_kafka_topics(self.streaming_collection_name) if self.reset else None\n",
    "        # Create kafka topics\n",
    "        self.create_kafka_topic(self.streaming_collection_name) if self.reset else None\n",
    "        \n",
    "        # Collection to be stored in MongoDB\n",
    "        self.warehouse_collection_dict = {collection_name: (self.db[collection_name], collection_name.split('_')[0]) for collection_name in self.warehouse_collection_name}\n",
    "        self.streaming_collection_dict = {collection_name: (self.db[collection_name], collection_name.split('_')[0]) for collection_name in self.streaming_collection_name}    \n",
    "        \n",
    "    def delete_kafka_topics(self, topics):\n",
    "        # Initialize the Kafka Admin Client\n",
    "        admin_client = confluent_kafka.admin.AdminClient(kafka_config)\n",
    "        \n",
    "        # Delete topics\n",
    "        fs = admin_client.delete_topics(topics, operation_timeout=30)\n",
    "        \n",
    "        # Wait for each operation to finish\n",
    "        for topic, f in fs.items():\n",
    "            try:\n",
    "                f.result()  # The result itself is None\n",
    "                logging.info(f\"Kafka topic {topic} deleted successfully\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to delete topic {topic}: {e}\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "    def create_kafka_topic(self, topics):\n",
    "        # Initialize the Kafka Admin Client\n",
    "        admin_client = confluent_kafka.admin.AdminClient(kafka_config)\n",
    "        new_topics = [confluent_kafka.admin.NewTopic(topic, num_partitions=6, replication_factor=3) for topic in topics]\n",
    "        # Create topics\n",
    "        fs = admin_client.create_topics(new_topics)\n",
    "        # Wait for each operation to finish\n",
    "        for topic, f in fs.items():\n",
    "            try:\n",
    "                f.result()  # The result itself is None\n",
    "                logging.info(f\"Kafka topic {topic} created successfully\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to create topic {topic}: {e}\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "    def createTimeSeriesCollection(self, collection_names: list, expireAfterSeconds=None):\n",
    "        \n",
    "        for collection_name in collection_names:\n",
    "            if collection_name not in self.db.list_collection_names():\n",
    "                self.db.create_collection(\n",
    "                    collection_name,\n",
    "                    timeseries={\n",
    "                        \"timeField\": \"date\" if 'datastream' not in collection_name else \"datetime\",  \n",
    "                        \"metaField\": \"symbol\",  \n",
    "                        \"granularity\": \"hours\" if 'datastream' not in collection_name else \"minutes\"\n",
    "                    },\n",
    "                    expireAfterSeconds = expireAfterSeconds if 'datastream' not in collection_name else None\n",
    "                )\n",
    "                logging.info(f\"Time Series Collection {collection_name} created successfully\")\n",
    "                time.sleep(3)\n",
    "            \n",
    "    def fetch_and_produce_stock_data(self):\n",
    "        for collection_name, (collection, interval) in self.warehouse_collection_dict.items():\n",
    "            for symbol in self.symbols:\n",
    "                try:\n",
    "                    # Fetch data from Yahoo Finance\n",
    "                    ticker = yf.Ticker(symbol)\n",
    "                    \n",
    "                    # Fetch all data if symbol does not exist in the collection\n",
    "                    if not collection.find_one({\"symbol\": symbol}):\n",
    "                        data = ticker.history(period='max', interval=interval).reset_index()\n",
    "                    else:\n",
    "                        # If the symbol exists, fetch data from the last date in the database\n",
    "                        logging.info(f\"{symbol} already exists in the database\")\n",
    "                        latest_record = collection.find_one({'symbol': symbol}, sort=[(\"date\", DESCENDING)])\n",
    "                        if latest_record:\n",
    "                            last_date_in_db = pd.to_datetime(latest_record['date']).strftime('%Y-%m-%d')\n",
    "                            if last_date_in_db == self.current_date:\n",
    "                                logging.info(f\"Data for {symbol} is up to date\")\n",
    "                                continue\n",
    "                            else:\n",
    "                                # Fetch only the missing data\n",
    "                                data = ticker.history(period='max', interval=interval).reset_index()\n",
    "                                data = data[data['Date'] > last_date_in_db]\n",
    "                                \n",
    "                                logging.info(f\"Fetching data for {symbol} from {last_date_in_db}\")\n",
    "                        else:\n",
    "                            logging.error(f\"Error fetching data for {symbol}\")\n",
    "                            continue\n",
    "\n",
    "                    # Produce data to Kafka\n",
    "                    if not data.empty: \n",
    "                        for _, record in data.iterrows():\n",
    "                            stock_record = {\n",
    "                                'symbol': symbol,\n",
    "                                'date': record['Date'].strftime('%Y-%m-%d'),\n",
    "                                'open': record['Open'],\n",
    "                                'high': record['High'],\n",
    "                                'low': record['Low'],\n",
    "                                'close': record['Close'],\n",
    "                                'volume': record['Volume']\n",
    "                            }\n",
    "                            # Serialize the record\n",
    "                            serilized_record = json.dumps(stock_record)\n",
    "                            # Send data to the respective topic\n",
    "                            self.producer.produce(topic=collection_name, value=serilized_record)\n",
    "            \n",
    "                        # Flush the producer after all messages have been sent\n",
    "                        self.producer.flush()\n",
    "                        # Log the success message\n",
    "                        logging.info(f\"Data for {symbol} until {stock_record['date']} sent successfully to {collection_name}\")                     \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error fetching data for {symbol}: {e}\")\n",
    "                    traceback.print_exc()\n",
    "                # Check the last reocrd date in the symbol\n",
    "                logging.info(f\"Fetching data for {symbol} completed!\")\n",
    "    \n",
    "    def fetch_and_produce_datastream(self):\n",
    "        for topic_name, (_, interval) in self.streaming_collection_dict.items():\n",
    "            for symbol in self.symbols:\n",
    "                try:\n",
    "                    ticker = yf.Ticker(symbol)\n",
    "                    if self.reset:\n",
    "                        start_date = self.current_date - pd.Timedelta(days=self.window_size[interval]) \n",
    "                        start_date = pd.to_datetime(start_date).strftime('%Y-%m-%d')\n",
    "                    elif not self.catch_up:\n",
    "                        # Set the start date to prescribed window size\n",
    "                        start_date = self.current_date - pd.Timedelta(days=self.window_size[interval]) \\\n",
    "                            if self.last_fetch[symbol][interval] is None \\\n",
    "                                else self.last_fetch[symbol][interval]\n",
    "                        # Increment the start date by one day to fetch today data\n",
    "                        start_date = (pd.to_datetime(start_date))\n",
    "                    elif self.catch_up:\n",
    "                        start_date = self.current_date - pd.Timedelta(days=self.window_size[interval])\n",
    "                        \n",
    "                    # Fetch data from Yahoo Finance\n",
    "                    data = ticker.history(start=start_date, interval=interval).reset_index()\n",
    "                    if self.reset:\n",
    "                        data = data[pd.to_datetime(data['Datetime']) >= start_date]\n",
    "                    elif not self.catch_up:\n",
    "                        # Fetch only the new data compared to the last fetch time\n",
    "                        last_fetch_time = pd.to_datetime(self.last_fetch[symbol][interval]).tz_localize('America/New_York')\n",
    "                        data = data[data['Datetime'] >= last_fetch_time]\n",
    "                        \n",
    "                    # Produce data to Kafka\n",
    "                    if not data.empty:\n",
    "                        for _, record in data.iterrows():\n",
    "                            stock_record = {\n",
    "                                'symbol': symbol,\n",
    "                                'datetime': record['Datetime'].strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                                'open': record['Open'],\n",
    "                                'high': record['High'],\n",
    "                                'low': record['Low'],\n",
    "                                'close': record['Close'],\n",
    "                                'volume': record['Volume']\n",
    "                            }\n",
    "                            \n",
    "                            serialized_record = json.dumps(stock_record)\n",
    "                            serialized_key = symbol.encode('utf-8')\n",
    "                            # Send data to the respective topic\n",
    "                            self.producer.produce(topic=topic_name, \n",
    "                                                    key=serialized_key,\n",
    "                                                    value=serialized_record)\n",
    "                            # Log the success message\n",
    "                            logging.info(f\"Data for {symbol} at {stock_record['datetime']} sent successfully to {topic_name}\")\n",
    "                            \n",
    "                        # Update the last fetch time within data fetching loop\n",
    "                        self.last_fetch[symbol][interval] = stock_record['datetime']\n",
    "                        \n",
    "                        # Flush the producer after all messages have been sent\n",
    "                        self.producer.flush()\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error fetching data for {symbol}: {e}\")\n",
    "                    traceback.print_exc()\n",
    "\n",
    "                # Write the last fetch record to data pipeline config file for future reference\n",
    "                with open('config/data_pipeline_config.yaml', 'r') as f:\n",
    "                    config = yaml.safe_load(f) or {}\n",
    "                    \n",
    "                config['data_extract']['last_fetch_records'] = self.last_fetch\n",
    "                \n",
    "                with open('config/data_pipeline_config.yaml', 'w') as f:\n",
    "                    yaml.safe_dump(config, f)\n",
    "                    \n",
    "        # Set Catch up to False after the first fetch\n",
    "        self.catch_up = False\n",
    "        \n",
    "    def delayed_fetch_and_produce(self):\n",
    "        # Introduce a delay before fetching to account for data latency\n",
    "        time.sleep(10)\n",
    "        self.fetch_and_produce_datastream()\n",
    "        \n",
    "    def run(self):\n",
    "        trading = True\n",
    "        current_hour = pd.to_datetime('now').hour\n",
    "        \n",
    "        if self.catch_up:\n",
    "            # self.fetch_and_produce_datastream()\n",
    "            self.fetch_and_produce_stock_data()\n",
    "            \n",
    "        elif not self.catch_up:\n",
    "            # Only set up schedules if before 14:00\n",
    "            if current_hour < 14:\n",
    "                # Loop through the minute intervals starting at 05 and incrementing by 5\n",
    "                for minute in range(5, 60, 5):\n",
    "                    schedule.every().hour.at(f\":{minute:02d}\").do(self.delayed_fetch_and_produce)\n",
    "                while trading:\n",
    "                    schedule.run_pending()\n",
    "                    time.sleep(1)\n",
    "                    if pd.to_datetime('now').hour >= 14:\n",
    "                        trading = False \n",
    "                \n",
    "                logging.info(\"Trading hour is over!\")\n",
    "                time.sleep(5)\n",
    "                # Consume and Ingest daily and weekly stock data\n",
    "                self.fetch_and_produce_stock_data()\n",
    "                logging.info(f\"Scheduled fetching and producing stock data at {self.current_date} completed!\")\n",
    "            else:\n",
    "                logging.info(\"Trading hour is over! Wait for the next trading day\")\n",
    "                time.sleep(1)\n",
    "            \n",
    "if __name__ == \"__main__\": \n",
    "    # Load Data Pipeline Configuration\n",
    "    data_pipeline_config = load_pipeline_config()\n",
    "    \n",
    "    # Load the MongoDB configuration\n",
    "    mongo_url = load_mongo_config()['url']\n",
    "    db_name = load_mongo_config()[\"db_name\"]\n",
    "    \n",
    "    # Load the streaming and warehouse interval\n",
    "    streaming_interval = load_mongo_config()[\"streaming_interval\"]\n",
    "    warehouse_interval = load_mongo_config()[\"warehouse_interval\"]\n",
    "    \n",
    "    # Load the Kafka configuration\n",
    "    kafka_config = load_kafka_config()\n",
    "    # List of stock symbols\n",
    "    stock_symbols = load_pipeline_config()['data_ingest']['desried_symbols']  # List of stock symbols\n",
    "    \n",
    "    # Initialize the StockDataExtractor\n",
    "    extractor = StockDataExtractor(symbols=stock_symbols,\n",
    "                                mongo_url=mongo_url,\n",
    "                                db_name=db_name,\n",
    "                                streaming_interval=streaming_interval,\n",
    "                                warehouse_interval=warehouse_interval,\n",
    "                                kafka_config=kafka_config,\n",
    "                                data_pipeline_config=data_pipeline_config)\n",
    "\n",
    "    extractor.run()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
